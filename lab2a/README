NAME: Rohan Varma
EMAIL: rvarm1@ucla.edu
ID: 111111111
1 slip day used

Files: 
lab2a_add.c - the file that implements adding with various locking in threads. 
lab2a_list.c: a driver for SortedList.c that manipulates the list in threads, with synchronization options
SortedList.c: an implementation of a sorted list according to the interface in SortedList.h
SortedList.h: the interface for a sorted list
Makefile: source for building all of  the specified targets
*.gp: provided code to generate plots
*.csv: data generated by running all of the spec's specified tests, which can be seen in the Makefile's tests target
*.png: the generated plots

Q 2.1.1: causing conflicts

It takes many iterations before errors are seen because with a small number of iterations (and since each thread spends only a small amount of time executing add), it is much less likely for both threads to enter add() at the same time. Once the iterations become larger, there are many more opportunities for each thread to enter add(), and as a result the likelihood of two threads racing in add() is increased. So as we increase the amount of iterations, our window for potential races become larger. Conversely, a very small number of iterations will rarely fail, because the race window is so small that the chance of both threads entering add() concurrently are much less. 

Q 2.1.2 - cost of yielding

The --yield runs are much slower because it introduces many more context switches. If sched_yield() is called then we need to context switch to schedule this process from running to ready, and then do another context switch to switch it back from ready to running. Since frequent context switches add time overhead, this is where the additional timing is going. 

With how we have implemented it currently, we do not get accurate timings because our time includes the overhead of context switching. If there were some way to very accurately measure each call to sched_yield() (and therefore the amount of time the associated context switches take), then we could subtract and get true per-operation timings. However, I do not believe that this is possible (ie, it can't be done with very precise accuracy) since caching very likely plays a role in sched_yield(), and it is very difficult to predict reliably the outcomes of the scheduler and cache. So I think it is not practically possible (though theoretically there is a way to do it). 


Q 2.1.3 - measurement errors

The average cost per iteration drops with increasing iterations because overhead plays less of  an effect in the total cost (time). The process of creating threads (ie, in the loop that does pthread_create) is roughly constant and does not depend on our number of iterations. So, if we have a small number of iterations then this overhead time is a larger fraction of the total time than if we had a large number of iterations. Therefore, to get the most "correct" cost we would like a large number of iterations, since as we increase the number of iterations, the ratio of time spent doing add()s to time spent for overhead increases, making the overhead time less and less relevant as the time spend doing add() operations dominates it. 

Q 2.1.4 -- costs of serialization
Since there is a small number of threads, it is less likely for threads to contend for the same resource and thus form a convoy. Even though only one thread can enter the critical section at a time, not much time is spent inside the critical section and therefore the lock is usually available to be acquired immediately (much more often than it not being available), so all locks (even the inefficient spin lock) do quite well.

The three protection operations slow down as the number of threads rises since there are many more threads competing for the same resource. Since only one thread can use the resource at once, if there are many threads it is likely that at least some of them will also want the resource, so they have to wait for it and thus a convoy is formed behind the thread that owns the resource and we lose parallelism. Essentially, there is increased contention for resources

Q 2.2.1 - scalability of mutex

In both the add and list graphs for mutex protection, the variation in time per mutex-protected op increases roughly linearly. For the adds, the increase is initially sharp but then it flattens out to slower, but still roughly linear increase. This slowing down is probably because of the increased contention for the mutex playing a role in addition to the overhead in each operation of locking and unlockign the mutex. On the other hand, the list's mutex cost per operation is roughly linear with a constant slope throughout. This makes sense since comparatively, list ops are more expensive than add operations, so the overhead of the mutex for each iteration plays a bigger role than the contention for the mutex/

Q 2.2.2

For a large number of threads, spin locks have much worse scalability than mutexes. For both add and list, after 2 threads the cost per op for spin locks increases much more shaply than it does for mutexes. The rate of increase for both spin locks and mutexes are roughly linear, but spin locks have a much larger slope. This is because using spin locks is much more expensive than using a mutex, since the spin lock wastes a lot of CPU time spinning when it doesn't have the lock. Since (especially for a large number of threads) we are in a high-contention environment, many threads could simultaneously be wasting a lot of cycles just spinning waiting for a resoruce. This doesn't happen with mutexes. If we only had to hold the lock for an extremely short interval and we knew that contention would be very rare, then a spin lock may be a choice, but it is not scalable in high-contention environments.
