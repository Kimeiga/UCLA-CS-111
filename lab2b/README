#NAME: Rohan Varma
#EMAIL: rvarm1@gmail.com
#ID: 111111111
1 slip day used
This submission contains:
SortedList.h: Interface for a sorted list
SortedList.c: Implements the methods in SortedList.h
lab2_list.c: Extends the lab2_list.c implementation to time how long it takes to acquire a lock, and to partition a list into sublists.
lab2_list.gp: A script that uses gnuplot to parse the csv file and produce the required plots
*.csv: The data generated by the generate script
*.png: The pictures (5) generate by the gnuplot script
Makefile: Contains the required targets for the program
README: Describes files and answers the questions
profile.out: execution profiling report showing where time spent in spinl lock implementation
Citations:
Jenkins one at a time hash function: https://en.wikipedia.org/wiki/Jenkins_hash_function#one-at-a-time
timespec difference implementation help: https://www.gnu.org/software/libc/manual/html_node/Elapsed-Time.html
Return value from pthread_join: http://stackoverflow.com/questions/13315575/c-pthread-join-return-value

Questions:

2.3.1 - Cycles in the basic list implementation

I believe that most of the cycles in the 1 or 2 threaded list program are dedicated to list lookups, inserts, deletes, and length operations. The locks are not under much contention, so for a smaller number of threads the list operations themselves are more expensive than, for example, spinning for a lock since the locks are not under contention. In the high-thread spin lock cases, I believe that most of the cycles are being spent on spin-waiting to acquire the spin lock. Since many threads are competing for the spin lock and only one thread can hold the spin lock, the rest of the threads will be using their CPU cycles to spin. For high-threaded mutex cases, most of the CPU cycles are being spent on list operations, because we don't need to use too many CPU cycles to acquire the lock, since each thread goes to sleep (and thus blocks and doesn't use the CPU) if it can't acquire the lock.

2.3.2 - Execution Profiling

The code that consumes the most CPU cycles is in the spin lock implementation, specifically spin_lock_and_time(), a function that is responsible for acquiring the spin lock and timing how long it takes to acquire. As expected, the line of code "while (__sync_lock_test_and_set(&mlist.lists[idx].spinlock, 1) == 1) ; //spin" takes up most of the CPU cycles in that function. This operation becomes prohibitively expensive for a large number of threads since there is high contention for the spin lock. Since only one thread can hold the spin lock at once and there are many threads contending for it, the rest of the n - 1 threads will keep spinning on the CPU, racing for the lock. The time a thread spins on the CPU for is potentially unbounded, since it's a race for the lock, there's no ordering.

2.3.3 - Mutx Waits

As the number of contending threads increases, the average lock wait time rises dramatically since there are more threads waiting for the lock in the queue. With a large amount of threads contending for the same lock, every thread besides the one holding the lock will be on the queue sleeping, which will collectively increase the time each thread spends waiting. Assuming a fair dequeuing policy, if we had 3 contending threads then the third thread would only have to wait for the 2 others to finish the critical section(s), while if we had 10 contending threads then the last thread would have to wait for 9 other threads, which is almost 5 times as much waiting.

The completion time per operation rises (less dramatically) with increasing contending threads because of caching effects. If there are many contending threads then it is likely that when one finally acquires the lock, the L1 and L2 caches will contain irrelevant data and main memory will needed to be accessed for instructions and data, and since L1 and L2 caches are much faster (but smaller) than main memory, this will increase the time it takes to do an operation. Also, the completion time per operation includes the CPU time that is spent in pthread_mutex_lock() which involves trying to acquire the lock and possibly putting the thread to sleep.

It is possible that wait time per op will increase faster than completion time per op in high-contention situations. The actual time it takes to complete a list operation increases slowly with contention because the time to do the operation is roughly constant, but the increased contention adds a bit of overhead due to the cpu time of mutex_lock(). However  it becomes significantly more and more expensive to actually get to run the operation because it is in the critical section, and there are several threads contending for the lock to enter the critical section. So each particular thread will end up spending more time waiting to do the operation because it doesn't have the lock, compared to the amount of time it takes to actually do the operation.

2.3.4 - Performance of Partitioned Lists

As we add more lists (from 1 to 16) we generally observe an increase in throughput - since there are more lists, there is less contention for a lock for a single list, and also the lists themselves are smaller. However, as we add more and more lists, we would not keep seeing significant gains in throughput because even though insert and delete become slightly less expensive, getting the length takes longer since we have to iterate through every list to get each individual length, so this would increase lock contention, and it would become significant with a larger number of lists. With a very large number of lists we will have several lists with only a few elements (or possibly none) and would still need to acquire the lock to get its length, when it could possibly be zero. We would also have a lot more lock overhead when we have a large amount of lists. 

According to the curves, it does seem reasonalbe to suggest that the throughput of a n-way partitioned list should be equivalent to the throughput of a single list with fewer threads. As an example, the throughput of a 16 way partitioned list with 12 threads is roughly equivalent (slightly less) to a 1-way partitioned list with 1 thread, for both mutex and spin synchronization. This makes sense, since if the proportion of lists to threads is approximately the same, then the throughput for those two situations should be similar, since each thread is responsible for the same amount of lists. For example, we would expend a 10 way partitioned list running on 2 threads to have approximately the same throughput as a 5 way partitioned list running on 1 thread, since each thread is responsible for 5 lists. If we examine the graphs closely, we see that the linear approximation for 1.33 threads and 1 lists corresponds very closely to the actual data point for 16 lists and 12 threads, which indicates that the proportionality idea may be correct. More performance analysis, with a larger number of threads and lists is needed to confirm this however. 
